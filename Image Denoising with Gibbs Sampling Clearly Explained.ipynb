{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Image Denoising with Gibb Sampling Clearly Explained"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4deb84ba44fb8db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's start!\n",
    "Let's start with some definition:\n",
    "We have a binary image $x$ of size $(N, M)$ with some noise. That means that each pixel $x_{ij}$ can be only white or black (1, -1) and could be flipped with a certain unknown probability. We'd like to have our actual denoised image that we'll denote as $y$. Notice that we will define in uppercase the random variables $X$ or $Y$ and the instances/sample of this random variables in lower case, $x$, $y$ respectively.\n",
    "We'll define the random variable $Y$ of the wanted denoised images to be composed by $N*M$ pixels, so $N*M$ random variables indexed as $Y_{ij}$ with $i$ and $j$, row and column respectively.\n",
    "\n",
    "Now we want to create our probability distribution $P(Y| X = x)$ that means: \"What is the probability distribution that defines Y given that the random variable X is exactly our image x?\". **We're doing this because we assume (or better we'd like) that if we sample from this distribution that most probable resulting images would be a denoised version of $x$. But, is this the best way? In this case, it'll be better to define a deterministic link between the probability distribution and the wanted denoised image. For example, we can set that if $P(Y_{ij} = 1 | Y_{\\backslash ij}, X = x) > 0.5 $ we'll set $ y_{ij} = 1$**.\n",
    "\n",
    "Ok, now we need to cope with at least two problems:\n",
    "1. **Distribution Definition Problem** \n",
    "    We need to define in some way this distribution, for example starting from some relations between the pixels. But once we have defined this distibution, are we able to compute it analitically? Or we need to estimate it?\n",
    "2. **Estimation of the Probability Problem**  \n",
    "    $Y$ is actually a multivariable distribution $Y_{ij}$ with $i \\in [0, N-1]$ and $j \\in [0, M-1]$, and it's possible that we would like to have the possibility to sample from it, but it's a multivariate distribution and this is a hard problem! $P(Y_{0,0}, ..., Y_{ij}, ... , Y_{N - 1,M - 1} | X = x)$.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa2dd5db971c8494"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Definition of a Probability Distribution that relates the pixels\n",
    "What do we have to do with $P(Y | X = x)$? As already mentioned, we can say that the best way to use it in a binary images problem, is to consider that if $P(Y_{ij} = 1 | Y_{\\backslash ij}, X = x) > 0.5 $ so $ y_{ij} = 1$. We can use this approach because considering a stable distribution we have in this way one and only one final denoised image. Because otherwise we can sample from the distribution undefinetely having multiple probably not good resulting image.\n",
    "Perfect! But do we have analitically this distribution? Probably not, because, as we'll see later, we have $N*M$ unknown conditionally dependent variables. But are we able to sample from it? YES! And how to do it is the answer to the final problem to solve. \n",
    "So a good way is to sample multiple times from $P(Y_{ij} = 1 |Y_{\\backslash ij}, X = x)$, estimate it's value and use it to define $y_{ij} = 1$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8f9f5cfa5f4b4f3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### From an equation to a probability distribution\n",
    "\n",
    "It's reasonable to think that each pixel can depend only on its neighbours let's say, following the image below:\n",
    "\n",
    "![alternative text](img.png)\n",
    "\n",
    "So, we can impose that the random variable $Y_{ij}$ depends on $Y_{i-1,j}, Y_{i+1, j}, Y_{i, j-1}, Y_{i, j+1}$, that are the four neighbours, that we'll call $N(Y_{ij})$ and $X_{ij}$ that is the corresponding noisy pixel."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "495dc26d8c87fc61"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, now we can say that it is of course easier using a \"deterministic function\" instead of a probabilistic ones.\n",
    "In a binary image, where the pixels can be only black or white, we can set a some kind of voting system where pixel should have a high probability to be equal to the neighbours pixels. For example, we can define some rules:\n",
    "1. The function should give a high value when all the surrounding pixels are the same;\n",
    "2. The function should give a low value when all the surronding pixels are different, messy let's say.(high entropy?)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b856b198ef569f42"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If for example we already have a proposal for the pixel $Y_{ij} = y_{ij}$ and all its neighbours $N(Y_{ij})$, and that the values of these pixels can be {1, -1}. How can we evaluate this proposal? Just a \"Multiplication\" is a good proposal!! \n",
    "$$\n",
    "    E(Y_{ij} = y_{ij}, N(Y_{ij}) = N'(y_{ij}), X_{ij} = x_{ij}) = \\alpha*y_{ij}x_{ij} + \\beta*\\sum_{ij \\in N(y_{ij})} y_{ij}y_{i'j'}\n",
    "$$\n",
    "Using $\\alpha$ and $\\beta$ as hyperparameters. \n",
    "Why this equation could work?? Well, if all the values are 1, $E = \\alpha + \\beta*|N(y_{ij})|$, but also if all the values are -1 we have $E = \\alpha + \\beta*|N(y_{ij})|$. So, reflecting the first rule that we gave. Instead, if $\\alpha = \\beta$ to simplify, we have the lowest value of E when three over five values are different. Of course this depends also on the hyperparameters $\\alpha$ and $\\beta$.\n",
    "Yes, Yes, but we need a probability distribution, here we have a deterministic equation instead and moreover we're assuming we already have all the values. So??"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fa8291e71eaeec6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preliminaries: Boltzmann Distribution\n",
    "A nice way to transform any function in a probability distribution is using the Boltzmann distribution that is in the form of:\n",
    "$$\n",
    "    P(X) = \\frac{e^{-E(X)}}{\\sum_{X} e^{-E(X)}} = \\frac{e^{-E(X)}}{Z}\n",
    "\n",
    "$$\n",
    "Where $E(X)$ is called Energy and it's the function that we'd like to use to model our problem. In this case **The lower the energy the highest the probability, but we have defined our energy function in the opposite way!!! So we need to fix it! A minus sign is enough :)**\n",
    "$$\n",
    "    E(Y_{ij} = y_{ij}, N(Y_{ij}) = N'(y_{ij}), X_{ij} = x_{ij}) = - (\\alpha*y_{ij}x_{ij} + \\beta*\\sum_{ij \\in N(y_{ij})} y_{ij}y_{i'j'})\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "deaa22998d1f59bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we go! A problem is solved. But we need to think for a moment.\n",
    "Remember that actually we only have $X = x$, but $Y$ and all its pixels are still random variables, so we'll have:\n",
    "$$\n",
    "    P(Y_{ij}, N(Y_{ij}) |X_{ij} = x) = \\frac{e^{-E(Y_{ij}, N(Y_{ij}) | X_{ij} = x_{ij})}}{\\sum_{values \\in Y_{ij}, N(Y_{ij})} e^{-E(Y_{ij}, N(Y_{ij}) | X_{ij} = x_{ij})}} = \\frac{e^{-E(y_{ij}, N(y_{ij}) | X_{ij} = x_{ij})}}{Z}\n",
    "$$\n",
    "But to make the things easier we want to focus on the single pixel so computing $P(Y_{ij} |N(Y_{ij}) = N'(Y_{ij}), X_{ij} = x)$ like if we had some samples of the neighbours $N'(Y_{ij})$ (we'll see later how we can assume this).\n",
    "$$\n",
    "    P(Y_{ij} |N(Y_{ij}) = N'(Y_{ij}), X_{ij} = x) = \\frac{P(Y_{ij}, N(Y_{ij}) | X_{ij} = x)}{P(N(Y_{ij}) | X_{ij} = x)} = \\frac{P(Y_{ij}, N(Y_{ij}) | X_{ij} = x)}{\\sum_{ config \\sim Y_{ij}}P(Y_{ij}, N(Y_{ij}) | X_{ij} = x)} = \\frac{\\frac{e^{-E(Y_{ij}, N(Y_{ij}) | X_{ij} = x_{ij})}}{Z}}{\\sum_{ config \\sim Y_{ij}}\\frac{e^{-E(Y_{ij}, N(y_{ij}) | X_{ij} = x_{ij})}}{Z}} = \\\\\n",
    "        \\frac{e^{-E(Y_{ij}, N(Y_{ij}) | X_{ij} = x_{ij})}}{\\sum_{ config \\sim Y_{ij}} e^{-E(Y_{ij}, N(Y_{ij}) | X_{ij} = x_{ij})}} = \\frac{e^{-E(Y_{ij}, N(Y_{ij}) | X_{ij} = x_{ij})}}{ e^{-E(Y_{ij} = 1, N(Y_{ij}) | X_{ij} = x_{ij})} + e^{-E(Y_{ij} = -1, N(y_{ij}) | X_{ij} = x_{ij})}}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe947643a0a47b22"
  },
  {
   "cell_type": "markdown",
   "source": [
    "So,\n",
    "$$\n",
    "    -E(Y_{ij} = 1, N(Y_{ij}) | X_{ij} = x_{ij}) = \\alpha*x_{ij} + \\beta*\\sum_{ij \\in N(Y_{ij})} Y_{i'j'} \\\\\n",
    "    -E(Y_{ij} = -1, N(Y_{ij}) | X_{ij} = x_{ij}) =  - (\\alpha*x_{ij} + \\beta*\\sum_{ij \\in N(Y_{ij})} Y_{i'j'})\n",
    "$$\n",
    "And,\n",
    "$$\n",
    "P(Y_{ij} |N(Y_{ij}) = N'(Y_{ij}), X_{ij} = x) = \\frac{e^{\\alpha*y_{ij}x_{ij} + \\beta*\\sum_{ij \\in N(y_{ij})} y_{ij}y_{i'j'})}}{ e^{ \\alpha*x_{ij} + \\beta*\\sum_{ij \\in N(Y_{ij})} y_{i'j'}} + e^{-(\\alpha*x_{ij} + \\beta*\\sum_{ij \\in N(Y_{ij})} y_{i'j'})}}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b3c2e6e1c7dc05b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To still simplify the computation, let's say we would like to compute only when $Y_{ij} = 1$,\n",
    "$$\n",
    "P(Y_{ij} = 1 |N(Y_{ij}) = N'(Y_{ij}), X_{ij} = x) = \\frac{e^{\\alpha*x_{ij} + \\beta*\\sum_{ij \\in N(y_{ij})} y_{i'j'})}}{ e^{\\alpha*x_{ij} + \\beta*\\sum_{ij \\in N(Y_{ij})} y_{i'j'}} + e^{-(\\alpha*x_{ij} + \\beta*\\sum_{ij \\in N(Y_{ij})} y_{i'j'})}}\n",
    "$$\n",
    "Reminding that:\n",
    "$$\n",
    "    \\frac{e^{a}}{e^{a} + e^{-a}} = \\frac{e^a}{ e^a + \\frac{1}{e^a} } = \\frac{e^{2a}}{1 + e^{2a}} = \\frac{1}{1 + e^{-2a}}\n",
    "$$\n",
    "we finally have:\n",
    "\n",
    "$$\n",
    "P(Y_{ij} = 1 |N(Y_{ij}) = N'(Y_{ij}), X_{ij} = x) = \\frac{1}{1 + e^{-2(\\alpha*x_{ij} + \\beta*\\sum_{ij \\in N(Y_{ij})} y_{i'j'})}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef80659bc2def14e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Okay... and now??"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6266e38c03c1265f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Estimate the Probability values for each pixels\n",
    "Gibbs sampling is a Markov Chain Monte Carlo algorithm for sampling for a specified multivariate probability distribution when direct sampling from a joint distribution is difficult, but sampling from a conditional distribution is more practical. \n",
    "Let's say we have $X = (X_1, ..., X_n)$ a multivariate random variable with $p(x_1, ..., x_n)$ joint distribution and we want to obtain $k$ samples from it. We proceed as follows:\n",
    "1. We begin with some initial value $X^{(0)}$\n",
    "2. We'd like the next sample $X^{(i+1)}$, but it's multivariate, and the components can be conditioned each other. Hence, we sample each component conditioned on all the other components sampled so far. But there is a catch: we sample our $j$th component conditioned till the $j-1$th. That is, sampling from $p(X_j^{i+1} | X_1^{i+1}, ..., X_{j-1}^{i+1}, X_{j+1}^{i}, ..., X_{N}^{i})$\n",
    "3. Repeat the process $k$ times.\n",
    "\n",
    "**After a certain burn-in period these samples are like being obtained from the joint distribution**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca5b47ad577d1470"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's rewind for a moment!\n",
    "We remember that we want $P(Y | X = x)$ and that $Y$ is multivariate distribution with interdependent variables ( since we defined that each variables is dependent from its four neighbours) and it would be good if it would be possible to estimate them, that's exactly the $X$ presented in the Gibbs Sampling algorithm. Morever, we already have a conditional distribution formula to use (of course because I explained everything backward :P). Hence:\n",
    "1. Run a Gibbs Sampler for our $Y$ for a burn-in period\n",
    "2. Take $k$ samples from each $P(Y_{ij}| ...) \\forall ij$\n",
    "3. Estimate $P(Y_{ij} = 1 \\ ...)$ from the $k$ samples: $P(Y_{ij} = 1 \\ ...) \\approx \\frac{1}{K} \\sum_{k=1}^{K} y_{ij}$ with $y_{ij} = 1$\n",
    "4. Set the denoised pixel as 1 if $P(Y_{ij} = 1 \\ ...) > 0.5$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f45aee792444938"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's start to code!\n",
    "At first we need to import our noisy image $x$.\n",
    "Note that since we are considering the four neighbours, we have a problem on the borders in which case the neighbours will be only 2! \n",
    "We can solve this issue just adding a bit of padding :)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "844ef116f430db48"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "def load_image(filename):\n",
    "    my_img = plt.imread(filename)\n",
    "    img_gray = np.dot(my_img[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "    img_gray = np.where(img_gray > 0.5, 1, -1)\n",
    "    img_padded = np.zeros([img_gray.shape[0] + 2, img_gray.shape[1] + 2]) ## Add padding\n",
    "    img_padded[1:-1, 1:-1] = img_gray\n",
    "    return img_padded"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T15:40:35.664166237Z",
     "start_time": "2024-01-28T15:40:35.606087890Z"
    }
   },
   "id": "486a0cc59224bd72",
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "As described in the Gibb Sampling Algorithm a random initialization for our $Y$ it's enough."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "105438390625d74"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X = load_image(\"image.png\")\n",
    "Y = np.random.choice([1, -1], size=X.shape) ## random init"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T15:40:35.665767861Z",
     "start_time": "2024-01-28T15:40:35.664523364Z"
    }
   },
   "id": "6e06be45e85a4551",
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to define the posterior for the single component of $Y$. We report the function for simplicity:\n",
    "$$\n",
    "P(Y_{ij} = 1 |N(Y_{ij}) = N'(Y_{ij}), X_{ij} = x) = \\frac{1}{1 + e^{-2(\\alpha*x_{ij} + \\beta*\\sum_{ij \\in N(Y_{ij})} y_{i'j'})}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf6307a4253ce290"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import math\n",
    "def sample_y(i, j, Y, X, alpha=1, beta=1):\n",
    "    markov_blanket = [Y[i - 1, j], Y[i, j - 1], Y[i, j + 1], Y[i + 1, j], X[i, j]]\n",
    "    w = alpha*markov_blanket[-1] + beta*sum(markov_blanket[:4])\n",
    "    prob = 1 / (1 + math.exp(-2*w))\n",
    "    return (np.random.rand() < prob) * 2 - 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T15:40:35.673223820Z",
     "start_time": "2024-01-28T15:40:35.669046229Z"
    }
   },
   "id": "6d932d97bde4be52",
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, now we need to develop the Gibb sampler. First we need to define the **Burn-in period** and the **Sample size** that we will use to estimate each $P(Y_{ij} = 1 | N(Y_{ij}, X_{ij} = x_{ij})$. So, for this reason a for loop is enough.\n",
    "\n",
    "Then remember from the Gibbs sampling algorithm the we need to sample from $p(X_j^{i+1} | X_1^{i+1}, ..., X_{j-1}^{i+1}, X_{j+1}^{i}, ..., X_{N}^{i})$, and updating a pixel at time the whole image. So, we use a double loop (one for the rows, one for the column) and save each pixel sample in a temporary storage.\n",
    "\n",
    "We expect, that after a certain **burn-in** period, the whole image sampled is like has been sampled from $P(Y | X=x)$, that is exactly what we are looking for.\n",
    "In fact, after the burn-in period we save the whole images sampled.\n",
    "Actually in our example, since the image is binary, it's easier to just save how many times we find that the pixel is one, then diving by the sample size ( remember $P(Y_{ij} = 1 \\ ...) \\approx \\frac{1}{K} \\sum_{k=1}^{K} y_{ij}$ with $y_{ij} = 1$)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbc7fe94b3f0f302"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "def gibbs_sampler(X, Y, K = 10, n_burn_in_steps=10):\n",
    "    posterior = np.zeros(X.shape)\n",
    "    for step in tqdm(range(n_burn_in_steps + K)):\n",
    "        for i in range(1, X.shape[0] - 1):\n",
    "            for j in range(1, X.shape[1] - 1):\n",
    "                y = sample_y(i, j, Y, X)\n",
    "                Y[i,j] = y\n",
    "                if step > n_burn_in_steps and y == 1:\n",
    "                    posterior[i, j] += 1\n",
    "    posterior /= K\n",
    "    return posterior"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T15:40:35.681052456Z",
     "start_time": "2024-01-28T15:40:35.673396397Z"
    }
   },
   "id": "2a8999234486485b",
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Now we can start our computation!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77887c33dcc97efb"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 62/70 [00:20<00:02,  2.80it/s]"
     ]
    }
   ],
   "source": [
    "posterior = gibbs_sampler(X, Y, K= 20, n_burn_in_steps=50)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-28T15:40:35.722763607Z"
    }
   },
   "id": "35cb65ab47737acf",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have estimated our $P(Y_{ij} | ...) \\forall ij$ we can extract our image following $P(Y_{ij} | ...) > 0.5$ then $y_{ij} = 1$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a5344778a474c70"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "posterior[posterior > 0.5] = 1\n",
    "plt.imshow(posterior, cmap=\"binary\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b5c1845dcd930021",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# And....We're DONE!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6dca05068fce930"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Some additional notes: Maximum A Posteriori\n",
    "Let's suppose that we have a sample of values $x_1, ..., x_n$ of a random variable $X$ that follow a certain probability distribution $P(X)$ that actually depends on some parameters, let's say $\\theta$. Our problem now is to find the parameter that better explains the sample, so maximizing $P(X=x_1, X=x_2, ..., X=x_n | \\theta)$. To deal with this problem we have different possibilities (MLE is an example). MAP, instead sees this problem in the different way. Using the Bayes theorem:\n",
    "$$\n",
    "    P(\\theta | X) = \\frac{P(X|\\theta)P(\\theta)}{P(X)}\n",
    "$$\n",
    "Of course here we are considering that $\\theta$ is a random variable, with a certain prior distribution $P(\\theta)$ (that in general is unknown and fixed in some way, there are a lot of debates on its choice of course). So, here we are saying that considering that we have a certain sample $x_1, ..., x_n$, we should have a probability distribution of \\theta with a mode that better explains \"why we have that sample\" and we want to find the value of that mode:\n",
    "$$\n",
    "    \\hat{\\theta}_{MAP} = \\underset{\\theta}{\\operatorname{argmax}} \\frac{P(X|\\theta)P(\\theta)}{P(X)}\n",
    "$$\n",
    "Considering that we are not looking for a probability distribution, but a single value we can get rid of the denominator\n",
    "$$\n",
    "    \\hat{\\theta}_{MAP} = \\underset{\\theta}{\\operatorname{argmax}} P(X|\\theta)P(\\theta)\n",
    "$$\n",
    "Considering i.i.d samples and applying the log\n",
    "$$\n",
    "    \\hat{\\theta}_{MAP} = \\underset{\\theta}{\\operatorname{argmax}} \\prod_i^nP(X_i|\\theta)P(\\theta) \n",
    "        = \\underset{\\theta}{\\operatorname{argmax}} log \\prod_i^nP(X = x_i|\\theta)P(\\theta) \n",
    "        = \\underset{\\theta}{\\operatorname{argmax}} \\sum_i^n log P(X = x_i|\\theta)P(\\theta) \n",
    "        = \\underset{\\theta}{\\operatorname{argmax}} \\sum_i^n log P(X = x_i|\\theta) + log P(\\theta) \n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89acc1433e72cecd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's go back to our problem of the denoising of the image. We can consider our parameter $\\theta$ to be our random variable $Y$ of the denoised image. So asking what is the value of $Y$ that better explain our image $X$. But we can get a step back and remind that $ P(X|Y) = \\frac{P(X, Y)}{P(Y)}$, so $P(X,Y) = P(X|Y)P(Y)$. But wait we have $P(X,Y)$!!\n",
    "$$\n",
    "    P(X,Y) = \\frac{e^{\\sum_i^N \\sum_j^M (\\alpha*x_{ij}y_{ij} + \\beta* \\sum_{N(y_{ij})} y_{ij}y_{ij}')}}{Z}\n",
    "$$\n",
    "Where $E(X,Y) = \\sum_i^N \\sum_j^M (\\alpha*x_{ij}y_{ij} + \\beta* \\sum_{N(y_{ij})} y_{ij}y_{ij}')$ is the energy of the whole system. So,\n",
    "$$\n",
    "    \\hat{Y} = \\underset{y}{\\operatorname{argmax}} P(X|Y) P(Y) = \\underset{y}{\\operatorname{argmax}} P(X,Y) =  \\underset{y}{\\operatorname{argmax}} \\frac{e^{\\sum_i^N \\sum_j^M (\\alpha*x_{ij}y_{ij} + \\beta* \\sum_{N(y_{ij})} y_{ij}y_{ij}')}}{Z}\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be69666c683ce45c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the log trick (the points that maximize a certain function don't get affected if we apply the logarithm on it since it is a monotonic increasing function) and modifing the problem in a minimization problem, we get:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f81ee19e876515c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "    \\hat{Y} = \\underset{y}{\\operatorname{argmax}} \\frac{e^{\\sum_i^N \\sum_j^M (\\alpha*x_{ij}y_{ij} + \\beta* \\sum_{N(y_{ij})} y_{ij}y_{ij}')}}{Z} = \\underset{y}{\\operatorname{argmin}} - log \\frac{e^{\\sum_i^N \\sum_j^M (\\alpha*x_{ij}y_{ij} + \\beta* \\sum_{N(y_{ij})} y_{ij}y_{ij}')}}{Z} = - E(X,Y) + log(Z)\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cfcc7fbed932b6c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now besides the constant $Z$, we can see that the Gibbs sampling process tries to minimize the energy function. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b23996be73a23ef"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ba0a78db2c768f8d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def energy(X, Y, alpha=1, beta=1):\n",
    "    def get_energy(i, j, X, Y, alpha=1, beta=1):\n",
    "        return alpha*X[i, j]*Y[i,j] + beta*Y[i,j]*np.sum([Y[i - 1,j],  Y[i + 1,j],  Y[i,j - 1], Y[i,j + 1]])\n",
    "    energy = 0\n",
    "    for i in range(1, X.shape[0] - 1):\n",
    "        for j in range(1, X.shape[1]- 1):\n",
    "            energy += get_energy(i, j, X, Y)\n",
    "    return -energy"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "562fb1ae995f6ff9",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's modify the gibbs sampler to extract the energy:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77a5164a5f8a26e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "def gibbs_sampler_modified(X, Y, K = 10, n_burn_in_steps=10):\n",
    "    energy_history = []\n",
    "    posterior = np.zeros(X.shape)\n",
    "    for step in tqdm(range(n_burn_in_steps + K)):\n",
    "        for i in range(1, X.shape[0] - 1):\n",
    "            for j in range(1, X.shape[1] - 1):\n",
    "                y = sample_y(i, j, Y, X)\n",
    "                Y[i,j] = y\n",
    "                if step > n_burn_in_steps and y == 1:\n",
    "                    posterior[i, j] += 1\n",
    "        energy_history.append(energy(X,Y))\n",
    "    posterior /= K\n",
    "    return posterior, energy_history"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "47d5d80d39fa1403",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "Y = np.random.choice([1, -1], size=X.shape) ## random init\n",
    "posterior, energy_history = gibbs_sampler_modified(X, Y, K= 30, n_burn_in_steps=20)\n",
    "posterior[posterior > 0.5] = 1\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
    "fig.tight_layout(pad=2)\n",
    "ax[0].imshow(posterior, cmap=\"binary\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].plot(energy_history)\n",
    "ax[1].set_xlabel('Steps')\n",
    "ax[1].set_ylabel('Energy')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "80195f86e40d5272",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The End"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee62bad68983e992"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bibliography\n",
    "\n",
    "1. Main reference: Image Denoising with Gibbs Sampling (MCMC) https://towardsdatascience.com/image-denoising-with-gibbs-sampling-mcmc-concepts-and-code-implementation-11d42a90e153 \n",
    "2. https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation\n",
    "3. https://medium.com/@philipyunsoocho/image-denoising-by-gibbs-sampling-67d1ad3d7344"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f2cfc18cab34c3b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
